{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/NX-AI/tirex/blob/main/examples/quick_start_tirex.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open Quick Start In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install TiRex package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tirex-ts[notebooks,gluonts,hfdataset]' -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tirex import ForecastModel, load_model\n",
    "\n",
    "\n",
    "def plot_forecast(ctx, quantile_fc, real_future_values=None):\n",
    "    median_forecast = quantile_fc[:, 4].numpy()\n",
    "    lower_bound = quantile_fc[:, 0].numpy()\n",
    "    upper_bound = quantile_fc[:, 8].numpy()\n",
    "\n",
    "    original_x = range(len(ctx))\n",
    "    forecast_x = range(len(ctx), len(ctx) + len(median_forecast))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(original_x, ctx, label=\"Ground Truth Context\", color=\"#4a90d9\")\n",
    "    if real_future_values is not None:\n",
    "        original_fut_x = range(len(ctx), len(ctx) + len(real_future_values))\n",
    "        plt.plot(\n",
    "            original_fut_x,\n",
    "            real_future_values,\n",
    "            label=\"Ground Truth Future\",\n",
    "            color=\"#4a90d9\",\n",
    "            linestyle=\":\",\n",
    "        )\n",
    "    plt.plot(\n",
    "        forecast_x,\n",
    "        median_forecast,\n",
    "        label=\"Forecast (Median)\",\n",
    "        color=\"#d94e4e\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        forecast_x,\n",
    "        lower_bound,\n",
    "        upper_bound,\n",
    "        color=\"#d94e4e\",\n",
    "        alpha=0.1,\n",
    "        label=\"Forecast 10% - 90% Quantiles\",\n",
    "    )\n",
    "    plt.xlim(left=0)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "data_base_url = (\n",
    "    \"https://raw.githubusercontent.com/NX-AI/tirex/refs/heads/main/tests/data/\"\n",
    ")\n",
    "data_short = pd.read_csv(f\"{data_base_url}/air_passengers.csv\").values.reshape(-1)\n",
    "data_long = pd.read_csv(f\"{data_base_url}/loop_seattle_5T.csv\").values.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: ForecastModel = load_model(\"NX-AI/TiRex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short Horizon - Example\n",
    "ctx_s, future_s = np.split(data_short, [-12])\n",
    "quantiles, mean = model.forecast(ctx_s, prediction_length=24)\n",
    "plot_forecast(ctx_s, quantiles[0], future_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Horizon - Example\n",
    "ctx_l, future_l = np.split(data_long, [-512])\n",
    "quantiles, mean = model.forecast(ctx_l, prediction_length=768)\n",
    "plot_forecast(ctx_l, quantiles[0], future_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Options\n",
    "\n",
    "TiRex supports forecasting with different input types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data_short)\n",
    "\n",
    "# Torch tensor (2D or 1D)\n",
    "quantiles, means = model.forecast(context=data, prediction_length=24)\n",
    "print(\"Predictions (Torch tensor):\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "# List of Torch tensors (List of 1D) - will be padded\n",
    "list_torch_data = [data, data, data]\n",
    "quantiles, means = model.forecast(\n",
    "    context=list_torch_data, prediction_length=24, batch_size=2\n",
    ")\n",
    "print(\"Predictions (List of Torch tensors):\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "# NumPy array (2D or 1D)\n",
    "quantiles, means = model.forecast(\n",
    "    context=data.numpy(), prediction_length=24, output_type=\"torch\"\n",
    ")\n",
    "print(\"Predictions (NumPy):\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "# List of NumPy arrays (List of 1D) - will be padded\n",
    "list_numpy_data = [data.numpy()]  # Split into 3 sequences\n",
    "quantiles, means = model.forecast(context=list_numpy_data, prediction_length=24)\n",
    "print(\"Predictions (List of NumPy arrays):\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "# GluonTS Dataset\n",
    "try:\n",
    "    from typing import cast\n",
    "\n",
    "    from gluonts.dataset import Dataset\n",
    "\n",
    "    gluon_dataset = cast(\n",
    "        Dataset, [{\"target\": data, \"item_id\": 1}, {\"target\": data, \"item_id\": 22}]\n",
    "    )\n",
    "    quantiles, means = model.forecast_gluon(gluon_dataset, prediction_length=24)\n",
    "    print(\"Predictions GluonDataset:\\n\", type(quantiles), quantiles.shape)\n",
    "    # If you use also `glutonts` as your output type the start_time and item_id get preserved accordingly\n",
    "    predictions_gluon = model.forecast_gluon(\n",
    "        gluon_dataset, prediction_length=24, output_type=\"gluonts\"\n",
    "    )\n",
    "    print(\n",
    "        \"Predictions GluonDataset:\\n\",\n",
    "        type(predictions_gluon),\n",
    "        type(predictions_gluon[0]),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # To use the gluonts function you need to install the optional dependency\n",
    "    # pip install tirex[gluonts]\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Options\n",
    "\n",
    "\n",
    "TiRex supports different output types for the forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(data_short)\n",
    "\n",
    "# Default: 2D Torch tensor\n",
    "quantiles, means = model.forecast(\n",
    "    context=data, prediction_length=24, output_type=\"torch\"\n",
    ")\n",
    "print(\"Predictions:\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "# 2D Numpy Array\n",
    "quantiles, means = model.forecast(\n",
    "    context=data, prediction_length=24, output_type=\"numpy\"\n",
    ")\n",
    "print(\"Predictions:\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "# Iterate by patch\n",
    "# You can also use the forecast function as iterable. This might help with big datasets. All output_types are supported\n",
    "for i, fc_batch in enumerate(\n",
    "    model.forecast(\n",
    "        context=[data, data, data, data, data],\n",
    "        batch_size=2,\n",
    "        output_type=\"torch\",\n",
    "        yield_per_batch=True,\n",
    "    )\n",
    "):\n",
    "    quantiles, means = fc_batch\n",
    "    print(f\"Predictions batch {i}:\\n\", type(quantiles), quantiles.shape)\n",
    "\n",
    "\n",
    "try:\n",
    "    # QuantileForecast (GluonTS)\n",
    "    predictions_gluonts = model.forecast(\n",
    "        context=data, prediction_length=24, output_type=\"gluonts\"\n",
    "    )\n",
    "    print(\n",
    "        \"Predictions (GluonTS Quantile Forecast):\\n\",\n",
    "        type(predictions_gluon),\n",
    "        type(predictions_gluon[0]),\n",
    "    )\n",
    "    predictions_gluonts[0].plot()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # To use the gluonts function you need to install the optional dependency\n",
    "    # pip install tirex[gluonts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch compile\n",
    "Use `compile=True` to speed up the sLSTM layers when using the torch backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: ForecastModel = load_model(\n",
    "    \"NX-AI/TiRex\", backend=\"torch\", device=\"cuda\", compile=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
